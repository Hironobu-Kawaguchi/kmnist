{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_kmnist_pytorch_resnet50.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hironobu-Kawaguchi/kmnist/blob/master/colab_kmnist_pytorch_resnet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjL_okBrMNKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as dataset\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85E9lyoAMW8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        # x = F.relu(x)   # add\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfy7zHi9PqHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    __constants__ = ['downsample']\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    __constants__ = ['downsample']\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    # def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "    def __init__(self, block, layers, num_classes=10, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        # self.inplanes = 64\n",
        "        self.inplanes = 32\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        # self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "        self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfV0VuWCMc5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZT_rrnXMe4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF7EzJFkMhS-",
        "colab_type": "code",
        "outputId": "79da4285-1321-48ce-814a-46d8b33f3dc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Training settings\n",
        "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
        "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
        "                    help='input batch size for training (default: 64)')\n",
        "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
        "                    help='input batch size for testing (default: 1000)')\n",
        "parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
        "                    help='number of epochs to train (default: 14)')\n",
        "parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
        "                    help='learning rate (default: 1.0)')\n",
        "parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
        "                    help='Learning rate step gamma (default: 0.7)')\n",
        "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                    help='disables CUDA training')\n",
        "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                    help='random seed (default: 1)')\n",
        "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "                    help='how many batches to wait before logging training status')\n",
        "\n",
        "parser.add_argument('--save-model', action='store_true', default=True,\n",
        "                    help='For Saving the current Model')\n",
        "# args = parser.parse_args()\n",
        "args = parser.parse_args(args=[])\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.KMNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.KMNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=args.test_batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz to ../data/KMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "18169856it [00:05, 3117731.98it/s]                              \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/KMNIST/raw/train-images-idx3-ubyte.gz to ../data/KMNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz to ../data/KMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 48175.85it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/KMNIST/raw/train-labels-idx1-ubyte.gz to ../data/KMNIST/raw\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz to ../data/KMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3047424it [00:02, 1046800.17it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/KMNIST/raw/t10k-images-idx3-ubyte.gz to ../data/KMNIST/raw\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz to ../data/KMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 17664.36it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/KMNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/KMNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoZQAzf1OaBM",
        "colab_type": "code",
        "outputId": "dad32c78-922e-44a5-9b30-6dede6d73640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# model = Net().to(device)\n",
        "# model = ResNet(BasicBlock, [2, 2, 2, 2]).to(device)     # ResNet18\n",
        "model = ResNet(Bottleneck, [3, 4, 6, 3]).to(device)     # ResNet50\n",
        "model.eval() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQryQgYzOYBG",
        "colab_type": "code",
        "outputId": "bcc8d333-e2c1-4b65-e895-41bc8353332d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(args, model, device, train_loader, optimizer, epoch)\n",
        "    test(args, model, device, test_loader)\n",
        "    scheduler.step()\n",
        "\n",
        "if args.save_model:\n",
        "    torch.save(model.state_dict(), \"kmnist_cnn.pt\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.041850\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: -113.780655\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: -362.710236\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: -695.864441\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: -1259.304077\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: -2106.678223\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: -3795.682861\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: -6115.597168\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: -10381.652344\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: -15050.625977\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: -25773.832031\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: -38184.105469\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: -59016.222656\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: -91607.257812\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: -128622.250000\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: -196573.812500\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: -280786.687500\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: -385981.718750\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: -596451.312500\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: -804646.812500\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: -995080.000000\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: -1383972.250000\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: -1819822.250000\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: -2485369.250000\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: -3277833.250000\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: -3950929.500000\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: -5475050.000000\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: -6962173.500000\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: -8466293.000000\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: -10389418.000000\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: -13184105.000000\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: -16011463.000000\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: -20503326.000000\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: -25009442.000000\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: -32425404.000000\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: -39238184.000000\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: -44763168.000000\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: -56565516.000000\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: -64810832.000000\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: -79474880.000000\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: -104336264.000000\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: -110902608.000000\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: -141323760.000000\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: -146953184.000000\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: -190250256.000000\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: -228592096.000000\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: -273621056.000000\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: -302594752.000000\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: -363424288.000000\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: -418545760.000000\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: -467516384.000000\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: -582389504.000000\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: -649720896.000000\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: -756758272.000000\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: -825750720.000000\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: -1003217152.000000\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: -1151089536.000000\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: -1346761984.000000\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: -1474467200.000000\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: -1641113216.000000\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: -1899673600.000000\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: -1976051072.000000\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: -2355029504.000000\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: -2911828480.000000\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: -3114046720.000000\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: -3083429888.000000\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: -3723602944.000000\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: -3868343552.000000\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: -4938673664.000000\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: -5261983744.000000\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: -5619162624.000000\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: -6982873088.000000\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: -7184182272.000000\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: -9133918208.000000\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: -7757691904.000000\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: -9397833728.000000\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: -11492761600.000000\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: -13057812480.000000\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: -13406879744.000000\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: -13226362880.000000\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: -14529476608.000000\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: -18036576256.000000\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: -18377822208.000000\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: -17264867328.000000\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: -18870755328.000000\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: -26489028608.000000\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: -27162492928.000000\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: -30615156736.000000\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: -30750486528.000000\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: -30088505344.000000\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: -38640582656.000000\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: -36219768832.000000\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: -46209875968.000000\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: -43274510336.000000\n",
            "\n",
            "Test set: Average loss: -46446586678.4768, Accuracy: 1972/10000 (20%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: -53339250688.000000\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: -53216542720.000000\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: -44491456512.000000\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: -47567880192.000000\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: -67318943744.000000\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: -62617735168.000000\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: -73552338944.000000\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: -72537038848.000000\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: -66919710720.000000\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: -78125711360.000000\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: -86988595200.000000\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: -68251332608.000000\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: -92506742784.000000\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: -99819053056.000000\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: -96404283392.000000\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: -91223588864.000000\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: -118297255936.000000\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: -108739100672.000000\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: -110734467072.000000\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: -135399194624.000000\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: -103495630848.000000\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: -150348726272.000000\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: -146684198912.000000\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: -134529859584.000000\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: -166707281920.000000\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: -180394409984.000000\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: -149199667200.000000\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: -185292537856.000000\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: -175895150592.000000\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: -201466429440.000000\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: -205993525248.000000\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: -207959801856.000000\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: -204687589376.000000\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: -188993617920.000000\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: -271688548352.000000\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: -229557813248.000000\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: -198807814144.000000\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: -266627923968.000000\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: -290022850560.000000\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: -320808321024.000000\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: -283865579520.000000\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: -264182038528.000000\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: -319773442048.000000\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: -350048059392.000000\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: -356763533312.000000\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: -303345664000.000000\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: -380181577728.000000\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: -329238708224.000000\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: -423674675200.000000\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: -467460915200.000000\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: -386087616512.000000\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: -462956527616.000000\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: -445847764992.000000\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: -416904871936.000000\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: -388240539648.000000\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: -445627432960.000000\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: -469058125824.000000\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: -530395463680.000000\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: -559897509888.000000\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: -515893133312.000000\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: -547885416448.000000\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: -602459144192.000000\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: -438838951936.000000\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: -642900885504.000000\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: -614451904512.000000\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: -616166457344.000000\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: -733595238400.000000\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: -750870790144.000000\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: -650976231424.000000\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: -798327898112.000000\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: -732484665344.000000\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: -863776800768.000000\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: -764468461568.000000\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: -837395021824.000000\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: -774007881728.000000\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: -1008775921664.000000\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: -774256656384.000000\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: -940520505344.000000\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: -1001224667136.000000\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: -1039375859712.000000\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: -937557753856.000000\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: -959092162560.000000\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: -1307927379968.000000\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: -1057644544000.000000\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: -1010760548352.000000\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: -1007872966656.000000\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: -1304982978560.000000\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: -1208477417472.000000\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: -1413096407040.000000\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: -1167517810688.000000\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: -1313539227648.000000\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: -1208236900352.000000\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: -1357319503872.000000\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: -1384523759616.000000\n",
            "\n",
            "Test set: Average loss: -1320650581789.9009, Accuracy: 1979/10000 (20%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: -1185708376064.000000\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: -1416586723328.000000\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: -1785071796224.000000\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: -1620154646528.000000\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: -1826495791104.000000\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: -1409996161024.000000\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: -1921746206720.000000\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: -1621037154304.000000\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: -1756963930112.000000\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: -1205453062144.000000\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: -1967958654976.000000\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: -2027659067392.000000\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: -1896692056064.000000\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: -1841609179136.000000\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: -1373033332736.000000\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: -1412855365632.000000\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: -1787799928832.000000\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: -2235813855232.000000\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: -1785293832192.000000\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: -2194336120832.000000\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: -2133394194432.000000\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: -1559861919744.000000\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: -1909979742208.000000\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: -1862728417280.000000\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: -1598971576320.000000\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: -2369769177088.000000\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: -2203520073728.000000\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: -1640865988608.000000\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: -2407851622400.000000\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: -2528722026496.000000\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: -2362143670272.000000\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: -2571475091456.000000\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: -2354643206144.000000\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: -2746125385728.000000\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: -2874324287488.000000\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: -2460404154368.000000\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: -2870603677696.000000\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: -2372650401792.000000\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: -2493862379520.000000\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: -2136861048832.000000\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: -2526117101568.000000\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: -1701815779328.000000\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: -2652280455168.000000\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: -2967972872192.000000\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: -2724887789568.000000\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: -3113782083584.000000\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: -2850001518592.000000\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: -2445501792256.000000\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: -2851834429440.000000\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: -2785899970560.000000\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: -2931469058048.000000\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: -2980509122560.000000\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: -3032482578432.000000\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: -3488919584768.000000\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: -2845099163648.000000\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: -3743030968320.000000\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: -3398382911488.000000\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: -2311845838848.000000\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: -2873164038144.000000\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: -3904989560832.000000\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: -3323043512320.000000\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: -3689056567296.000000\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: -3057889312768.000000\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: -3658160799744.000000\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: -3052847497216.000000\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: -4132172988416.000000\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: -3881437233152.000000\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: -4550991806464.000000\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: -3280345235456.000000\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: -3391375540224.000000\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: -4112196567040.000000\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: -3098972520448.000000\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: -3374131445760.000000\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: -3939259383808.000000\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: -4066622046208.000000\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: -3436687654912.000000\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: -3930652409856.000000\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: -3643181891584.000000\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: -3336039301120.000000\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: -4254538661888.000000\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: -5264582377472.000000\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: -3326846697472.000000\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: -4961694384128.000000\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: -4747811094528.000000\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: -4192976502784.000000\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: -4379889369088.000000\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: -4589557383168.000000\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: -4840734326784.000000\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: -5294657110016.000000\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: -3681648377856.000000\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: -4693194964992.000000\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: -4840478998528.000000\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: -5094861963264.000000\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: -3742840651776.000000\n",
            "\n",
            "Test set: Average loss: -4507143539104.1533, Accuracy: 3231/10000 (32%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: -4883364708352.000000\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: -3773208723456.000000\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: -4998120865792.000000\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: -4446012047360.000000\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: -5082932838400.000000\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: -4432952557568.000000\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: -5215902236672.000000\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: -4913148461056.000000\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: -5144447549440.000000\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: -5021972299776.000000\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: -5214812241920.000000\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: -4910106542080.000000\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: -3829742960640.000000\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: -5354411261952.000000\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: -4666171588608.000000\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: -6027265703936.000000\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: -5131212423168.000000\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: -5735841267712.000000\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: -5725103849472.000000\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: -3812919607296.000000\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: -4444890071040.000000\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: -5490977800192.000000\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: -5732244652032.000000\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: -4753060790272.000000\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: -5561532809216.000000\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: -6125339017216.000000\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: -5821126148096.000000\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: -4939337170944.000000\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: -4623687483392.000000\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: -5055548227584.000000\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: -5113251889152.000000\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: -4049942347776.000000\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: -5534095769600.000000\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: -5496089083904.000000\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: -6832968957952.000000\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: -5833263939584.000000\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: -5383893549056.000000\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: -6420571881472.000000\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: -5887659868160.000000\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: -6122847600640.000000\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: -6072983093248.000000\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: -6073949880320.000000\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: -5485536215040.000000\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: -6253208666112.000000\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: -5650336186368.000000\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: -5589310636032.000000\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: -6434964635648.000000\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: -5296427630592.000000\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: -6000319922176.000000\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: -5720326012928.000000\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: -6920473673728.000000\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: -7333603704832.000000\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: -7084020072448.000000\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: -5761930362880.000000\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: -7174365904896.000000\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: -5673263824896.000000\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: -6936967249920.000000\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: -6942263083008.000000\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: -6464799768576.000000\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: -5506810249216.000000\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: -5407533170688.000000\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: -3979779244032.000000\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: -6582988439552.000000\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: -7664249077760.000000\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: -5987575005184.000000\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: -6850764865536.000000\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: -6870400499712.000000\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: -6042144997376.000000\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: -6416360275968.000000\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: -6767383674880.000000\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: -7147889885184.000000\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: -6737256513536.000000\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: -5874382798848.000000\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: -4997941559296.000000\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: -7521101676544.000000\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: -6131665600512.000000\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: -5594944110592.000000\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: -7280642228224.000000\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: -6576165355520.000000\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: -6474863476736.000000\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: -5998190788608.000000\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: -7411839008768.000000\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: -7632706338816.000000\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: -8424810610688.000000\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: -5500149170176.000000\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: -8518132301824.000000\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: -9324776652800.000000\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: -5686459105280.000000\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: -8016914546688.000000\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: -6283690770432.000000\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: -8200453619712.000000\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: -7002936311808.000000\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: -8924799434752.000000\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: -5587801735168.000000\n",
            "\n",
            "Test set: Average loss: -7238023681304.1660, Accuracy: 3692/10000 (37%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: -8372994179072.000000\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: -7677560750080.000000\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: -8687389769728.000000\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: -8895267340288.000000\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: -7855520874496.000000\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: -6565393858560.000000\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: -7258034929664.000000\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: -9040159571968.000000\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: -6911839174656.000000\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: -8941568262144.000000\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: -7937647443968.000000\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: -8082315280384.000000\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: -6663354449920.000000\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: -6614324084736.000000\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: -7404748013568.000000\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: -9248583974912.000000\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: -8181711372288.000000\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: -8983131717632.000000\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: -8200587837440.000000\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: -7825387421696.000000\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: -9342325620736.000000\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: -6451585613824.000000\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: -6234295500800.000000\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: -5542075432960.000000\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: -8615263469568.000000\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: -8492252397568.000000\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: -8048201433088.000000\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: -6294554017792.000000\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: -8158207541248.000000\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: -9101467713536.000000\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: -8801487945728.000000\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: -8202841751552.000000\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: -8393954689024.000000\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: -7744590970880.000000\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: -9341090398208.000000\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: -9563422064640.000000\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: -9889493549056.000000\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: -8783514304512.000000\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: -9444573315072.000000\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: -7864175820800.000000\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: -7859082362880.000000\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: -7469030440960.000000\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: -6072179884032.000000\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: -8766386339840.000000\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: -7210186309632.000000\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: -9194220552192.000000\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: -7218563383296.000000\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: -9033360605184.000000\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: -8120704172032.000000\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: -6457165086720.000000\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: -8608158318592.000000\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: -9057949712384.000000\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: -8563087900672.000000\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: -8524193595392.000000\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: -7879455145984.000000\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: -9119805210624.000000\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: -9017627770880.000000\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: -7827224002560.000000\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: -8189905469440.000000\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: -8292867768320.000000\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: -7768360091648.000000\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: -9223941390336.000000\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: -7323117944832.000000\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: -9333880389632.000000\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: -5286023659520.000000\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: -8422344359936.000000\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: -8539816329216.000000\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: -8159969148928.000000\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: -10527091720192.000000\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: -8365805666304.000000\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: -8346512916480.000000\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: -8021768929280.000000\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: -10039669555200.000000\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: -7636266778624.000000\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: -9100458983424.000000\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: -9583822110720.000000\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: -8338601934848.000000\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: -7269966675968.000000\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: -9354078060544.000000\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: -7007697371136.000000\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: -8479496994816.000000\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: -8503891066880.000000\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: -7032105074688.000000\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: -7235790438400.000000\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: -8611768565760.000000\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: -8457467461632.000000\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: -10106652590080.000000\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: -9675106942976.000000\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: -10049896316928.000000\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: -9566522703872.000000\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: -7866464862208.000000\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: -10697718104064.000000\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: -10151354433536.000000\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: -8291483648000.000000\n",
            "\n",
            "Test set: Average loss: -8319501546710.6309, Accuracy: 3769/10000 (38%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: -7922184093696.000000\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: -9651045269504.000000\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: -9230047248384.000000\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: -8308351041536.000000\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: -10078517198848.000000\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: -8471505272832.000000\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: -8110593277952.000000\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: -8594684641280.000000\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: -10106649444352.000000\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: -10696624439296.000000\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: -10431668158464.000000\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: -5861242044416.000000\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: -7851720835072.000000\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: -8045355073536.000000\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: -9328860856320.000000\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: -8672377831424.000000\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: -10517219377152.000000\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: -11174306381824.000000\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: -8269952188416.000000\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: -9346960326656.000000\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: -10406699466752.000000\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: -7079195574272.000000\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: -9497561006080.000000\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: -11027499450368.000000\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: -8622331920384.000000\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: -9560252219392.000000\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: -10320449896448.000000\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: -10899515506688.000000\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: -12173383303168.000000\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: -10671709224960.000000\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: -11005086138368.000000\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: -10544500178944.000000\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: -7292684075008.000000\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: -8873110929408.000000\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: -11000320360448.000000\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: -8943683239936.000000\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: -8306100797440.000000\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: -8569036996608.000000\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: -10219662868480.000000\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: -7157992390656.000000\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: -10385504600064.000000\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: -6803837419520.000000\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: -9868702384128.000000\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: -10166417227776.000000\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: -8646052282368.000000\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: -8508375302144.000000\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: -10403650207744.000000\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: -8137383346176.000000\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: -10203528429568.000000\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: -5025836302336.000000\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: -10895691350016.000000\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: -10256528703488.000000\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: -8802383429632.000000\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: -10005620195328.000000\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: -8511753814016.000000\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: -8991680757760.000000\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: -8975256911872.000000\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: -8238017806336.000000\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: -9859053387776.000000\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: -8616852062208.000000\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: -10149045469184.000000\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: -8969444655104.000000\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: -10827639816192.000000\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: -10178141356032.000000\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: -7552885587968.000000\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: -9652691533824.000000\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: -9018887110656.000000\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: -11779682861056.000000\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: -11034086604800.000000\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: -7769391890432.000000\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: -10582270935040.000000\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: -10232320229376.000000\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: -8881269899264.000000\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: -8316344336384.000000\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: -9815767121920.000000\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: -8825474121728.000000\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: -10836111261696.000000\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: -10594084192256.000000\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: -10832165470208.000000\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: -9394715623424.000000\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: -7446450929664.000000\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: -11555345268736.000000\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: -9560699961344.000000\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: -10161179590656.000000\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: -6407705853952.000000\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: -8630464151552.000000\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: -10375205486592.000000\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: -10127122890752.000000\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: -9704557248512.000000\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: -8485187092480.000000\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: -9353375514624.000000\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: -9989644091392.000000\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: -9048844926976.000000\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: -7656879161344.000000\n",
            "\n",
            "Test set: Average loss: -9342250190372.8633, Accuracy: 3782/10000 (38%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: -12050200788992.000000\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: -11063925932032.000000\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: -7330263990272.000000\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: -9832104984576.000000\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: -9392854401024.000000\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: -9857003421696.000000\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: -11785016967168.000000\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: -11135225954304.000000\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: -10557522444288.000000\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: -9399240228864.000000\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: -10291158974464.000000\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: -8096452706304.000000\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: -10962876760064.000000\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: -9932394987520.000000\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: -10174089658368.000000\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: -10541718306816.000000\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: -9303352147968.000000\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: -10089298657280.000000\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: -10095885811712.000000\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: -9607560822784.000000\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: -10564865622016.000000\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: -8769275166720.000000\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: -9863131299840.000000\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: -9471010013184.000000\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: -11471317630976.000000\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: -10686397677568.000000\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: -8090832338944.000000\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: -8744506228736.000000\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: -10386261671936.000000\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: -9665193705472.000000\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: -13485834502144.000000\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: -8849966759936.000000\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: -9187963699200.000000\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: -6475840749568.000000\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: -9882624327680.000000\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: -9781390606336.000000\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: -10264285020160.000000\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: -9524561838080.000000\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: -10682965688320.000000\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: -10694826131456.000000\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: -8223124881408.000000\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: -11132364390400.000000\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: -9652026736640.000000\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: -8373451358208.000000\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: -12718144749568.000000\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: -11102569103360.000000\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: -11592413478912.000000\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: -9851549777920.000000\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: -8420535042048.000000\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: -11078828294144.000000\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: -9501167058944.000000\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: -11146358685696.000000\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: -9038544764928.000000\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: -10672820715520.000000\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: -8525783760896.000000\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: -8030691786752.000000\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: -11550950686720.000000\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: -9405147906048.000000\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: -5845318369280.000000\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: -9146543898624.000000\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: -7903504236544.000000\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: -10899621412864.000000\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: -10870113435648.000000\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: -7889285545984.000000\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: -9217929904128.000000\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: -10311870447616.000000\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: -8584625127424.000000\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: -10195272990720.000000\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: -10065662705664.000000\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: -9151082135552.000000\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: -10949193891840.000000\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: -10830354579456.000000\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: -9098843127808.000000\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: -11162542407680.000000\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: -6413288996864.000000\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: -10073905561600.000000\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: -9922523693056.000000\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: -11479255351296.000000\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: -12127691603968.000000\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: -10525609033728.000000\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: -11858491736064.000000\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: -11763374358528.000000\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: -12138567434240.000000\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: -11418683310080.000000\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: -10918001901568.000000\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: -10558248058880.000000\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: -8589497335808.000000\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: -10971454111744.000000\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: -8899088351232.000000\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: -11531096948736.000000\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: -11367702593536.000000\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: -10809373622272.000000\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: -9771770970112.000000\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: -10377832169472.000000\n",
            "\n",
            "Test set: Average loss: -10005707317732.9668, Accuracy: 3812/10000 (38%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: -11261982015488.000000\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: -11642472497152.000000\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: -7925122727936.000000\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: -10555837382656.000000\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: -9500437250048.000000\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: -11767829757952.000000\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: -9093397872640.000000\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: -9490783010816.000000\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: -10222790770688.000000\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: -10311006420992.000000\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: -10649575882752.000000\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: -10325350940672.000000\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: -10822063489024.000000\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: -12084571013120.000000\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: -8141866008576.000000\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: -12657921884160.000000\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: -12099315040256.000000\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: -9117162799104.000000\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: -8733281222656.000000\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: -8161272528896.000000\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: -11679862620160.000000\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: -10067874152448.000000\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: -9452823511040.000000\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: -10264774705152.000000\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: -9931908448256.000000\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: -9928137768960.000000\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: -10061185286144.000000\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: -12053649555456.000000\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: -8624558571520.000000\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: -9781562572800.000000\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: -10863092170752.000000\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: -9283844440064.000000\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: -11370989879296.000000\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: -9651413319680.000000\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: -10088983035904.000000\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: -13431823400960.000000\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: -12420085972992.000000\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: -9697421688832.000000\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: -7957881290752.000000\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: -10287761588224.000000\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: -9554508120064.000000\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: -9635683631104.000000\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: -8833387724800.000000\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: -10516558774272.000000\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: -11535084683264.000000\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: -10941853859840.000000\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: -10694306037760.000000\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: -8751949545472.000000\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: -11608345542656.000000\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: -12190302076928.000000\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: -11243833262080.000000\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: -9963980193792.000000\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: -9023983190016.000000\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: -10966178725888.000000\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: -11430636027904.000000\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: -11046437781504.000000\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: -10450632704000.000000\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: -11515116650496.000000\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: -6387764559872.000000\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: -11868091449344.000000\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: -11368894824448.000000\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: -8499123191808.000000\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: -7914385309696.000000\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: -11794121752576.000000\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: -10322173755392.000000\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: -9769671720960.000000\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: -8758927294464.000000\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: -11356277309440.000000\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: -11349546500096.000000\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: -9686362357760.000000\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: -11928423366656.000000\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: -10565183340544.000000\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: -11454972428288.000000\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: -11853363150848.000000\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: -12035937009664.000000\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: -11694152613888.000000\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: -9229163298816.000000\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: -9028906254336.000000\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: -9984803864576.000000\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: -9855291097088.000000\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: -10593891254272.000000\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: -12658870845440.000000\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: -11444235010048.000000\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: -9705330049024.000000\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: -9569245855744.000000\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: -10904456396800.000000\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: -10376416591872.000000\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: -11356683108352.000000\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: -11918487060480.000000\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: -8329545383936.000000\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: -12540414263296.000000\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: -10455830495232.000000\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: -10155214241792.000000\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: -10464684670976.000000\n",
            "\n",
            "Test set: Average loss: -10127329195143.9863, Accuracy: 3813/10000 (38%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: -9243281326080.000000\n",
            "Train Epoch: 9 [640/60000 (1%)]\tLoss: -9482396499968.000000\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: -10147962290176.000000\n",
            "Train Epoch: 9 [1920/60000 (3%)]\tLoss: -12336674897920.000000\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: -6660192468992.000000\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: -8821919449088.000000\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: -9272012308480.000000\n",
            "Train Epoch: 9 [4480/60000 (7%)]\tLoss: -13351553859584.000000\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: -8996344823808.000000\n",
            "Train Epoch: 9 [5760/60000 (10%)]\tLoss: -11448882298880.000000\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: -9295999533056.000000\n",
            "Train Epoch: 9 [7040/60000 (12%)]\tLoss: -13277385981952.000000\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: -11523913154560.000000\n",
            "Train Epoch: 9 [8320/60000 (14%)]\tLoss: -9924478238720.000000\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: -10889220587520.000000\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: -11201158316032.000000\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: -11271144472576.000000\n",
            "Train Epoch: 9 [10880/60000 (18%)]\tLoss: -12763779825664.000000\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: -12922984071168.000000\n",
            "Train Epoch: 9 [12160/60000 (20%)]\tLoss: -9917601677312.000000\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: -11541553348608.000000\n",
            "Train Epoch: 9 [13440/60000 (22%)]\tLoss: -11919987572736.000000\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: -9249924055040.000000\n",
            "Train Epoch: 9 [14720/60000 (25%)]\tLoss: -8462306639872.000000\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: -11504610967552.000000\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: -10483336740864.000000\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: -13115894792192.000000\n",
            "Train Epoch: 9 [17280/60000 (29%)]\tLoss: -9749690056704.000000\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: -9882709262336.000000\n",
            "Train Epoch: 9 [18560/60000 (31%)]\tLoss: -11700031979520.000000\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: -8600406196224.000000\n",
            "Train Epoch: 9 [19840/60000 (33%)]\tLoss: -9710577123328.000000\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: -10931514900480.000000\n",
            "Train Epoch: 9 [21120/60000 (35%)]\tLoss: -11304998797312.000000\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: -7715466248192.000000\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: -8839552303104.000000\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: -9824755515392.000000\n",
            "Train Epoch: 9 [23680/60000 (39%)]\tLoss: -9241177882624.000000\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: -10966619127808.000000\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: -11103163645952.000000\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: -11686749667328.000000\n",
            "Train Epoch: 9 [26240/60000 (44%)]\tLoss: -11414641049600.000000\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: -10512486105088.000000\n",
            "Train Epoch: 9 [27520/60000 (46%)]\tLoss: -7088984031232.000000\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: -10519671996416.000000\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: -9362120638464.000000\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: -12383137300480.000000\n",
            "Train Epoch: 9 [30080/60000 (50%)]\tLoss: -11626537287680.000000\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: -10257858297856.000000\n",
            "Train Epoch: 9 [31360/60000 (52%)]\tLoss: -9495200661504.000000\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: -9282173009920.000000\n",
            "Train Epoch: 9 [32640/60000 (54%)]\tLoss: -8820509114368.000000\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: -9152245006336.000000\n",
            "Train Epoch: 9 [33920/60000 (57%)]\tLoss: -9800628830208.000000\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: -7936304218112.000000\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: -9359169945600.000000\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: -9786878853120.000000\n",
            "Train Epoch: 9 [36480/60000 (61%)]\tLoss: -12743512948736.000000\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: -11268610064384.000000\n",
            "Train Epoch: 9 [37760/60000 (63%)]\tLoss: -11546837123072.000000\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: -9355448549376.000000\n",
            "Train Epoch: 9 [39040/60000 (65%)]\tLoss: -11921799512064.000000\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: -9515172888576.000000\n",
            "Train Epoch: 9 [40320/60000 (67%)]\tLoss: -8369880432640.000000\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: -11064961925120.000000\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: -12056003608576.000000\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: -12369302388736.000000\n",
            "Train Epoch: 9 [42880/60000 (71%)]\tLoss: -9423685681152.000000\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: -10470689865728.000000\n",
            "Train Epoch: 9 [44160/60000 (74%)]\tLoss: -7150542782464.000000\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: -8533084471296.000000\n",
            "Train Epoch: 9 [45440/60000 (76%)]\tLoss: -8805196759040.000000\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: -9293774454784.000000\n",
            "Train Epoch: 9 [46720/60000 (78%)]\tLoss: -10033553211392.000000\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: -10150100336640.000000\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: -11627714838528.000000\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: -11536067198976.000000\n",
            "Train Epoch: 9 [49280/60000 (82%)]\tLoss: -9972267089920.000000\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: -8252338208768.000000\n",
            "Train Epoch: 9 [50560/60000 (84%)]\tLoss: -9125972934656.000000\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: -12010816274432.000000\n",
            "Train Epoch: 9 [51840/60000 (86%)]\tLoss: -10555660173312.000000\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: -9811837059072.000000\n",
            "Train Epoch: 9 [53120/60000 (88%)]\tLoss: -9601795751936.000000\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: -12913583587328.000000\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: -11866452525056.000000\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: -6609846140928.000000\n",
            "Train Epoch: 9 [55680/60000 (93%)]\tLoss: -11470087651328.000000\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: -11589449154560.000000\n",
            "Train Epoch: 9 [56960/60000 (95%)]\tLoss: -10109215309824.000000\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: -10472645459968.000000\n",
            "Train Epoch: 9 [58240/60000 (97%)]\tLoss: -10880583467008.000000\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: -12092155363328.000000\n",
            "Train Epoch: 9 [59520/60000 (99%)]\tLoss: -8488337014784.000000\n",
            "\n",
            "Test set: Average loss: -9406739178009.3945, Accuracy: 3807/10000 (38%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: -5782268542976.000000\n",
            "Train Epoch: 10 [640/60000 (1%)]\tLoss: -10916966957056.000000\n",
            "Train Epoch: 10 [1280/60000 (2%)]\tLoss: -10053506564096.000000\n",
            "Train Epoch: 10 [1920/60000 (3%)]\tLoss: -10545325408256.000000\n",
            "Train Epoch: 10 [2560/60000 (4%)]\tLoss: -9051845951488.000000\n",
            "Train Epoch: 10 [3200/60000 (5%)]\tLoss: -9409266712576.000000\n",
            "Train Epoch: 10 [3840/60000 (6%)]\tLoss: -11649134100480.000000\n",
            "Train Epoch: 10 [4480/60000 (7%)]\tLoss: -8602027294720.000000\n",
            "Train Epoch: 10 [5120/60000 (9%)]\tLoss: -11475249790976.000000\n",
            "Train Epoch: 10 [5760/60000 (10%)]\tLoss: -9942654255104.000000\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: -10547354402816.000000\n",
            "Train Epoch: 10 [7040/60000 (12%)]\tLoss: -9544070594560.000000\n",
            "Train Epoch: 10 [7680/60000 (13%)]\tLoss: -8159472648192.000000\n",
            "Train Epoch: 10 [8320/60000 (14%)]\tLoss: -11181944209408.000000\n",
            "Train Epoch: 10 [8960/60000 (15%)]\tLoss: -9053780574208.000000\n",
            "Train Epoch: 10 [9600/60000 (16%)]\tLoss: -10379908349952.000000\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: -11809503313920.000000\n",
            "Train Epoch: 10 [10880/60000 (18%)]\tLoss: -13359726460928.000000\n",
            "Train Epoch: 10 [11520/60000 (19%)]\tLoss: -11963113406464.000000\n",
            "Train Epoch: 10 [12160/60000 (20%)]\tLoss: -11638919921664.000000\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: -7913927081984.000000\n",
            "Train Epoch: 10 [13440/60000 (22%)]\tLoss: -11141192351744.000000\n",
            "Train Epoch: 10 [14080/60000 (23%)]\tLoss: -6010221101056.000000\n",
            "Train Epoch: 10 [14720/60000 (25%)]\tLoss: -9949895720960.000000\n",
            "Train Epoch: 10 [15360/60000 (26%)]\tLoss: -9071875850240.000000\n",
            "Train Epoch: 10 [16000/60000 (27%)]\tLoss: -12084312014848.000000\n",
            "Train Epoch: 10 [16640/60000 (28%)]\tLoss: -12444664594432.000000\n",
            "Train Epoch: 10 [17280/60000 (29%)]\tLoss: -10165442052096.000000\n",
            "Train Epoch: 10 [17920/60000 (30%)]\tLoss: -12045737000960.000000\n",
            "Train Epoch: 10 [18560/60000 (31%)]\tLoss: -8368682958848.000000\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: -9747726073856.000000\n",
            "Train Epoch: 10 [19840/60000 (33%)]\tLoss: -7898150207488.000000\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: -11572370997248.000000\n",
            "Train Epoch: 10 [21120/60000 (35%)]\tLoss: -9022428151808.000000\n",
            "Train Epoch: 10 [21760/60000 (36%)]\tLoss: -12394543710208.000000\n",
            "Train Epoch: 10 [22400/60000 (37%)]\tLoss: -9767094321152.000000\n",
            "Train Epoch: 10 [23040/60000 (38%)]\tLoss: -8244870774784.000000\n",
            "Train Epoch: 10 [23680/60000 (39%)]\tLoss: -12787170410496.000000\n",
            "Train Epoch: 10 [24320/60000 (41%)]\tLoss: -13057021444096.000000\n",
            "Train Epoch: 10 [24960/60000 (42%)]\tLoss: -8425967714304.000000\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: -9046253895680.000000\n",
            "Train Epoch: 10 [26240/60000 (44%)]\tLoss: -10175717048320.000000\n",
            "Train Epoch: 10 [26880/60000 (45%)]\tLoss: -13336960827392.000000\n",
            "Train Epoch: 10 [27520/60000 (46%)]\tLoss: -8766168236032.000000\n",
            "Train Epoch: 10 [28160/60000 (47%)]\tLoss: -7745039237120.000000\n",
            "Train Epoch: 10 [28800/60000 (48%)]\tLoss: -9197157613568.000000\n",
            "Train Epoch: 10 [29440/60000 (49%)]\tLoss: -11320791400448.000000\n",
            "Train Epoch: 10 [30080/60000 (50%)]\tLoss: -9460516913152.000000\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: -12095163727872.000000\n",
            "Train Epoch: 10 [31360/60000 (52%)]\tLoss: -8069717164032.000000\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: -10673106976768.000000\n",
            "Train Epoch: 10 [32640/60000 (54%)]\tLoss: -11136217907200.000000\n",
            "Train Epoch: 10 [33280/60000 (55%)]\tLoss: -10203990851584.000000\n",
            "Train Epoch: 10 [33920/60000 (57%)]\tLoss: -8385863352320.000000\n",
            "Train Epoch: 10 [34560/60000 (58%)]\tLoss: -12943753216000.000000\n",
            "Train Epoch: 10 [35200/60000 (59%)]\tLoss: -10423835295744.000000\n",
            "Train Epoch: 10 [35840/60000 (60%)]\tLoss: -12464099950592.000000\n",
            "Train Epoch: 10 [36480/60000 (61%)]\tLoss: -11706956775424.000000\n",
            "Train Epoch: 10 [37120/60000 (62%)]\tLoss: -10571462213632.000000\n",
            "Train Epoch: 10 [37760/60000 (63%)]\tLoss: -8922087817216.000000\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: -9555728662528.000000\n",
            "Train Epoch: 10 [39040/60000 (65%)]\tLoss: -12660915568640.000000\n",
            "Train Epoch: 10 [39680/60000 (66%)]\tLoss: -10727107592192.000000\n",
            "Train Epoch: 10 [40320/60000 (67%)]\tLoss: -8113292836864.000000\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: -12441358434304.000000\n",
            "Train Epoch: 10 [41600/60000 (69%)]\tLoss: -10728690941952.000000\n",
            "Train Epoch: 10 [42240/60000 (70%)]\tLoss: -11801896943616.000000\n",
            "Train Epoch: 10 [42880/60000 (71%)]\tLoss: -7563210915840.000000\n",
            "Train Epoch: 10 [43520/60000 (72%)]\tLoss: -10082092843008.000000\n",
            "Train Epoch: 10 [44160/60000 (74%)]\tLoss: -11725261766656.000000\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: -10133810708480.000000\n",
            "Train Epoch: 10 [45440/60000 (76%)]\tLoss: -10312945238016.000000\n",
            "Train Epoch: 10 [46080/60000 (77%)]\tLoss: -9644735987712.000000\n",
            "Train Epoch: 10 [46720/60000 (78%)]\tLoss: -11504125476864.000000\n",
            "Train Epoch: 10 [47360/60000 (79%)]\tLoss: -12399821193216.000000\n",
            "Train Epoch: 10 [48000/60000 (80%)]\tLoss: -11540669399040.000000\n",
            "Train Epoch: 10 [48640/60000 (81%)]\tLoss: -11923632422912.000000\n",
            "Train Epoch: 10 [49280/60000 (82%)]\tLoss: -11134983733248.000000\n",
            "Train Epoch: 10 [49920/60000 (83%)]\tLoss: -12086309552128.000000\n",
            "Train Epoch: 10 [50560/60000 (84%)]\tLoss: -8789520023552.000000\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: -10561858306048.000000\n",
            "Train Epoch: 10 [51840/60000 (86%)]\tLoss: -10237392191488.000000\n",
            "Train Epoch: 10 [52480/60000 (87%)]\tLoss: -11027910492160.000000\n",
            "Train Epoch: 10 [53120/60000 (88%)]\tLoss: -10073912901632.000000\n",
            "Train Epoch: 10 [53760/60000 (90%)]\tLoss: -8821746434048.000000\n",
            "Train Epoch: 10 [54400/60000 (91%)]\tLoss: -8475579514880.000000\n",
            "Train Epoch: 10 [55040/60000 (92%)]\tLoss: -10902313107456.000000\n",
            "Train Epoch: 10 [55680/60000 (93%)]\tLoss: -9674224041984.000000\n",
            "Train Epoch: 10 [56320/60000 (94%)]\tLoss: -12215070490624.000000\n",
            "Train Epoch: 10 [56960/60000 (95%)]\tLoss: -9247000625152.000000\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: -11468616499200.000000\n",
            "Train Epoch: 10 [58240/60000 (97%)]\tLoss: -11929862012928.000000\n",
            "Train Epoch: 10 [58880/60000 (98%)]\tLoss: -10713719373824.000000\n",
            "Train Epoch: 10 [59520/60000 (99%)]\tLoss: -8130440724480.000000\n",
            "\n",
            "Test set: Average loss: -9226020322279.4238, Accuracy: 3832/10000 (38%)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: -9176336039936.000000\n",
            "Train Epoch: 11 [640/60000 (1%)]\tLoss: -12287026921472.000000\n",
            "Train Epoch: 11 [1280/60000 (2%)]\tLoss: -12882931613696.000000\n",
            "Train Epoch: 11 [1920/60000 (3%)]\tLoss: -8748873023488.000000\n",
            "Train Epoch: 11 [2560/60000 (4%)]\tLoss: -12706346172416.000000\n",
            "Train Epoch: 11 [3200/60000 (5%)]\tLoss: -10006323789824.000000\n",
            "Train Epoch: 11 [3840/60000 (6%)]\tLoss: -12525785579520.000000\n",
            "Train Epoch: 11 [4480/60000 (7%)]\tLoss: -11323420180480.000000\n",
            "Train Epoch: 11 [5120/60000 (9%)]\tLoss: -8625392189440.000000\n",
            "Train Epoch: 11 [5760/60000 (10%)]\tLoss: -9161640247296.000000\n",
            "Train Epoch: 11 [6400/60000 (11%)]\tLoss: -11300055810048.000000\n",
            "Train Epoch: 11 [7040/60000 (12%)]\tLoss: -10283905974272.000000\n",
            "Train Epoch: 11 [7680/60000 (13%)]\tLoss: -13153163280384.000000\n",
            "Train Epoch: 11 [8320/60000 (14%)]\tLoss: -10039618174976.000000\n",
            "Train Epoch: 11 [8960/60000 (15%)]\tLoss: -10118874791936.000000\n",
            "Train Epoch: 11 [9600/60000 (16%)]\tLoss: -10136595726336.000000\n",
            "Train Epoch: 11 [10240/60000 (17%)]\tLoss: -8676572135424.000000\n",
            "Train Epoch: 11 [10880/60000 (18%)]\tLoss: -10808508547072.000000\n",
            "Train Epoch: 11 [11520/60000 (19%)]\tLoss: -11714946924544.000000\n",
            "Train Epoch: 11 [12160/60000 (20%)]\tLoss: -11054889304064.000000\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: -9495487971328.000000\n",
            "Train Epoch: 11 [13440/60000 (22%)]\tLoss: -2414376648704.000000\n",
            "Train Epoch: 11 [14080/60000 (23%)]\tLoss: -8866884485120.000000\n",
            "Train Epoch: 11 [14720/60000 (25%)]\tLoss: -11863695818752.000000\n",
            "Train Epoch: 11 [15360/60000 (26%)]\tLoss: -11303460536320.000000\n",
            "Train Epoch: 11 [16000/60000 (27%)]\tLoss: -11302468583424.000000\n",
            "Train Epoch: 11 [16640/60000 (28%)]\tLoss: -8178708774912.000000\n",
            "Train Epoch: 11 [17280/60000 (29%)]\tLoss: -9333787066368.000000\n",
            "Train Epoch: 11 [17920/60000 (30%)]\tLoss: -9644778979328.000000\n",
            "Train Epoch: 11 [18560/60000 (31%)]\tLoss: -11134836932608.000000\n",
            "Train Epoch: 11 [19200/60000 (32%)]\tLoss: -10208193544192.000000\n",
            "Train Epoch: 11 [19840/60000 (33%)]\tLoss: -13206316646400.000000\n",
            "Train Epoch: 11 [20480/60000 (34%)]\tLoss: -10638918156288.000000\n",
            "Train Epoch: 11 [21120/60000 (35%)]\tLoss: -8020828356608.000000\n",
            "Train Epoch: 11 [21760/60000 (36%)]\tLoss: -9551325691904.000000\n",
            "Train Epoch: 11 [22400/60000 (37%)]\tLoss: -8814708391936.000000\n",
            "Train Epoch: 11 [23040/60000 (38%)]\tLoss: -11446979133440.000000\n",
            "Train Epoch: 11 [23680/60000 (39%)]\tLoss: -12916669546496.000000\n",
            "Train Epoch: 11 [24320/60000 (41%)]\tLoss: -10449576787968.000000\n",
            "Train Epoch: 11 [24960/60000 (42%)]\tLoss: -9469263085568.000000\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: -10467963568128.000000\n",
            "Train Epoch: 11 [26240/60000 (44%)]\tLoss: -9501342171136.000000\n",
            "Train Epoch: 11 [26880/60000 (45%)]\tLoss: -11743836241920.000000\n",
            "Train Epoch: 11 [27520/60000 (46%)]\tLoss: -10371481993216.000000\n",
            "Train Epoch: 11 [28160/60000 (47%)]\tLoss: -10922434232320.000000\n",
            "Train Epoch: 11 [28800/60000 (48%)]\tLoss: -13426195693568.000000\n",
            "Train Epoch: 11 [29440/60000 (49%)]\tLoss: -11304414740480.000000\n",
            "Train Epoch: 11 [30080/60000 (50%)]\tLoss: -11821645824000.000000\n",
            "Train Epoch: 11 [30720/60000 (51%)]\tLoss: -10143828803584.000000\n",
            "Train Epoch: 11 [31360/60000 (52%)]\tLoss: -11815776944128.000000\n",
            "Train Epoch: 11 [32000/60000 (53%)]\tLoss: -9577470885888.000000\n",
            "Train Epoch: 11 [32640/60000 (54%)]\tLoss: -9795691085824.000000\n",
            "Train Epoch: 11 [33280/60000 (55%)]\tLoss: -13085461970944.000000\n",
            "Train Epoch: 11 [33920/60000 (57%)]\tLoss: -9740656574464.000000\n",
            "Train Epoch: 11 [34560/60000 (58%)]\tLoss: -10917603442688.000000\n",
            "Train Epoch: 11 [35200/60000 (59%)]\tLoss: -9643380178944.000000\n",
            "Train Epoch: 11 [35840/60000 (60%)]\tLoss: -10620001845248.000000\n",
            "Train Epoch: 11 [36480/60000 (61%)]\tLoss: -10388040056832.000000\n",
            "Train Epoch: 11 [37120/60000 (62%)]\tLoss: -8649502097408.000000\n",
            "Train Epoch: 11 [37760/60000 (63%)]\tLoss: -12140723306496.000000\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: -9416864694272.000000\n",
            "Train Epoch: 11 [39040/60000 (65%)]\tLoss: -10005286748160.000000\n",
            "Train Epoch: 11 [39680/60000 (66%)]\tLoss: -11321929105408.000000\n",
            "Train Epoch: 11 [40320/60000 (67%)]\tLoss: -10536068579328.000000\n",
            "Train Epoch: 11 [40960/60000 (68%)]\tLoss: -11024429219840.000000\n",
            "Train Epoch: 11 [41600/60000 (69%)]\tLoss: -11957932392448.000000\n",
            "Train Epoch: 11 [42240/60000 (70%)]\tLoss: -12092871540736.000000\n",
            "Train Epoch: 11 [42880/60000 (71%)]\tLoss: -10426939080704.000000\n",
            "Train Epoch: 11 [43520/60000 (72%)]\tLoss: -11542025207808.000000\n",
            "Train Epoch: 11 [44160/60000 (74%)]\tLoss: -12154269859840.000000\n",
            "Train Epoch: 11 [44800/60000 (75%)]\tLoss: -7768269914112.000000\n",
            "Train Epoch: 11 [45440/60000 (76%)]\tLoss: -9033950953472.000000\n",
            "Train Epoch: 11 [46080/60000 (77%)]\tLoss: -10333140811776.000000\n",
            "Train Epoch: 11 [46720/60000 (78%)]\tLoss: -9740198346752.000000\n",
            "Train Epoch: 11 [47360/60000 (79%)]\tLoss: -9391539486720.000000\n",
            "Train Epoch: 11 [48000/60000 (80%)]\tLoss: -11187177652224.000000\n",
            "Train Epoch: 11 [48640/60000 (81%)]\tLoss: -11078401523712.000000\n",
            "Train Epoch: 11 [49280/60000 (82%)]\tLoss: -7368548024320.000000\n",
            "Train Epoch: 11 [49920/60000 (83%)]\tLoss: -11314272403456.000000\n",
            "Train Epoch: 11 [50560/60000 (84%)]\tLoss: -12311519559680.000000\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: -7842642788352.000000\n",
            "Train Epoch: 11 [51840/60000 (86%)]\tLoss: -9402756104192.000000\n",
            "Train Epoch: 11 [52480/60000 (87%)]\tLoss: -9277390454784.000000\n",
            "Train Epoch: 11 [53120/60000 (88%)]\tLoss: -13027017490432.000000\n",
            "Train Epoch: 11 [53760/60000 (90%)]\tLoss: -11653080940544.000000\n",
            "Train Epoch: 11 [54400/60000 (91%)]\tLoss: -11499102797824.000000\n",
            "Train Epoch: 11 [55040/60000 (92%)]\tLoss: -11177059942400.000000\n",
            "Train Epoch: 11 [55680/60000 (93%)]\tLoss: -10276656119808.000000\n",
            "Train Epoch: 11 [56320/60000 (94%)]\tLoss: -12473673449472.000000\n",
            "Train Epoch: 11 [56960/60000 (95%)]\tLoss: -12306347982848.000000\n",
            "Train Epoch: 11 [57600/60000 (96%)]\tLoss: -10114309292032.000000\n",
            "Train Epoch: 11 [58240/60000 (97%)]\tLoss: -10562478014464.000000\n",
            "Train Epoch: 11 [58880/60000 (98%)]\tLoss: -10601273229312.000000\n",
            "Train Epoch: 11 [59520/60000 (99%)]\tLoss: -9128073232384.000000\n",
            "\n",
            "Test set: Average loss: -10537788804327.0137, Accuracy: 3821/10000 (38%)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: -7270925598720.000000\n",
            "Train Epoch: 12 [640/60000 (1%)]\tLoss: -11305910009856.000000\n",
            "Train Epoch: 12 [1280/60000 (2%)]\tLoss: -8763194998784.000000\n",
            "Train Epoch: 12 [1920/60000 (3%)]\tLoss: -10223102197760.000000\n",
            "Train Epoch: 12 [2560/60000 (4%)]\tLoss: -7129038585856.000000\n",
            "Train Epoch: 12 [3200/60000 (5%)]\tLoss: -10922122805248.000000\n",
            "Train Epoch: 12 [3840/60000 (6%)]\tLoss: -11401031581696.000000\n",
            "Train Epoch: 12 [4480/60000 (7%)]\tLoss: -11318182543360.000000\n",
            "Train Epoch: 12 [5120/60000 (9%)]\tLoss: -13516702482432.000000\n",
            "Train Epoch: 12 [5760/60000 (10%)]\tLoss: -12008167571456.000000\n",
            "Train Epoch: 12 [6400/60000 (11%)]\tLoss: -8814964244480.000000\n",
            "Train Epoch: 12 [7040/60000 (12%)]\tLoss: -7201410777088.000000\n",
            "Train Epoch: 12 [7680/60000 (13%)]\tLoss: -11144018264064.000000\n",
            "Train Epoch: 12 [8320/60000 (14%)]\tLoss: -10362999013376.000000\n",
            "Train Epoch: 12 [8960/60000 (15%)]\tLoss: -9804881854464.000000\n",
            "Train Epoch: 12 [9600/60000 (16%)]\tLoss: -9519667085312.000000\n",
            "Train Epoch: 12 [10240/60000 (17%)]\tLoss: -9180547121152.000000\n",
            "Train Epoch: 12 [10880/60000 (18%)]\tLoss: -9702682394624.000000\n",
            "Train Epoch: 12 [11520/60000 (19%)]\tLoss: -9695467143168.000000\n",
            "Train Epoch: 12 [12160/60000 (20%)]\tLoss: -10394021134336.000000\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: -9170403196928.000000\n",
            "Train Epoch: 12 [13440/60000 (22%)]\tLoss: -10957135806464.000000\n",
            "Train Epoch: 12 [14080/60000 (23%)]\tLoss: -9679764717568.000000\n",
            "Train Epoch: 12 [14720/60000 (25%)]\tLoss: -9758197153792.000000\n",
            "Train Epoch: 12 [15360/60000 (26%)]\tLoss: -11297221509120.000000\n",
            "Train Epoch: 12 [16000/60000 (27%)]\tLoss: -10758370885632.000000\n",
            "Train Epoch: 12 [16640/60000 (28%)]\tLoss: -10163101630464.000000\n",
            "Train Epoch: 12 [17280/60000 (29%)]\tLoss: -9941507112960.000000\n",
            "Train Epoch: 12 [17920/60000 (30%)]\tLoss: -10777496911872.000000\n",
            "Train Epoch: 12 [18560/60000 (31%)]\tLoss: -13109824585728.000000\n",
            "Train Epoch: 12 [19200/60000 (32%)]\tLoss: -12225165131776.000000\n",
            "Train Epoch: 12 [19840/60000 (33%)]\tLoss: -9790463934464.000000\n",
            "Train Epoch: 12 [20480/60000 (34%)]\tLoss: -9673905274880.000000\n",
            "Train Epoch: 12 [21120/60000 (35%)]\tLoss: -10483470958592.000000\n",
            "Train Epoch: 12 [21760/60000 (36%)]\tLoss: -8917685895168.000000\n",
            "Train Epoch: 12 [22400/60000 (37%)]\tLoss: -10566484623360.000000\n",
            "Train Epoch: 12 [23040/60000 (38%)]\tLoss: -10458059767808.000000\n",
            "Train Epoch: 12 [23680/60000 (39%)]\tLoss: -11446433873920.000000\n",
            "Train Epoch: 12 [24320/60000 (41%)]\tLoss: -10122396958720.000000\n",
            "Train Epoch: 12 [24960/60000 (42%)]\tLoss: -11290057637888.000000\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: -8455340949504.000000\n",
            "Train Epoch: 12 [26240/60000 (44%)]\tLoss: -10086211649536.000000\n",
            "Train Epoch: 12 [26880/60000 (45%)]\tLoss: -12196403740672.000000\n",
            "Train Epoch: 12 [27520/60000 (46%)]\tLoss: -10412848316416.000000\n",
            "Train Epoch: 12 [28160/60000 (47%)]\tLoss: -12401875353600.000000\n",
            "Train Epoch: 12 [28800/60000 (48%)]\tLoss: -10343239647232.000000\n",
            "Train Epoch: 12 [29440/60000 (49%)]\tLoss: -9888032882688.000000\n",
            "Train Epoch: 12 [30080/60000 (50%)]\tLoss: -10730679042048.000000\n",
            "Train Epoch: 12 [30720/60000 (51%)]\tLoss: -10399411863552.000000\n",
            "Train Epoch: 12 [31360/60000 (52%)]\tLoss: -12185797394432.000000\n",
            "Train Epoch: 12 [32000/60000 (53%)]\tLoss: -9861925437440.000000\n",
            "Train Epoch: 12 [32640/60000 (54%)]\tLoss: -10310207406080.000000\n",
            "Train Epoch: 12 [33280/60000 (55%)]\tLoss: -9742546108416.000000\n",
            "Train Epoch: 12 [33920/60000 (57%)]\tLoss: -12383838797824.000000\n",
            "Train Epoch: 12 [34560/60000 (58%)]\tLoss: -11386194231296.000000\n",
            "Train Epoch: 12 [35200/60000 (59%)]\tLoss: -11239910539264.000000\n",
            "Train Epoch: 12 [35840/60000 (60%)]\tLoss: -11389457399808.000000\n",
            "Train Epoch: 12 [36480/60000 (61%)]\tLoss: -10585447071744.000000\n",
            "Train Epoch: 12 [37120/60000 (62%)]\tLoss: -11596584714240.000000\n",
            "Train Epoch: 12 [37760/60000 (63%)]\tLoss: -10349256376320.000000\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: -7166182293504.000000\n",
            "Train Epoch: 12 [39040/60000 (65%)]\tLoss: -11340873728000.000000\n",
            "Train Epoch: 12 [39680/60000 (66%)]\tLoss: -8274639323136.000000\n",
            "Train Epoch: 12 [40320/60000 (67%)]\tLoss: -12238381383680.000000\n",
            "Train Epoch: 12 [40960/60000 (68%)]\tLoss: -11951308537856.000000\n",
            "Train Epoch: 12 [41600/60000 (69%)]\tLoss: -8210492162048.000000\n",
            "Train Epoch: 12 [42240/60000 (70%)]\tLoss: -9727853461504.000000\n",
            "Train Epoch: 12 [42880/60000 (71%)]\tLoss: -9807082815488.000000\n",
            "Train Epoch: 12 [43520/60000 (72%)]\tLoss: -11630000734208.000000\n",
            "Train Epoch: 12 [44160/60000 (74%)]\tLoss: -11413462450176.000000\n",
            "Train Epoch: 12 [44800/60000 (75%)]\tLoss: -11318983655424.000000\n",
            "Train Epoch: 12 [45440/60000 (76%)]\tLoss: -10584945852416.000000\n",
            "Train Epoch: 12 [46080/60000 (77%)]\tLoss: -10015236685824.000000\n",
            "Train Epoch: 12 [46720/60000 (78%)]\tLoss: -11276944146432.000000\n",
            "Train Epoch: 12 [47360/60000 (79%)]\tLoss: -9807437234176.000000\n",
            "Train Epoch: 12 [48000/60000 (80%)]\tLoss: -9416661270528.000000\n",
            "Train Epoch: 12 [48640/60000 (81%)]\tLoss: -11085070467072.000000\n",
            "Train Epoch: 12 [49280/60000 (82%)]\tLoss: -12092846374912.000000\n",
            "Train Epoch: 12 [49920/60000 (83%)]\tLoss: -8373336014848.000000\n",
            "Train Epoch: 12 [50560/60000 (84%)]\tLoss: -13022980472832.000000\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: -8618147053568.000000\n",
            "Train Epoch: 12 [51840/60000 (86%)]\tLoss: -11356734488576.000000\n",
            "Train Epoch: 12 [52480/60000 (87%)]\tLoss: -10246125780992.000000\n",
            "Train Epoch: 12 [53120/60000 (88%)]\tLoss: -11354556596224.000000\n",
            "Train Epoch: 12 [53760/60000 (90%)]\tLoss: -7467425071104.000000\n",
            "Train Epoch: 12 [54400/60000 (91%)]\tLoss: -10924041699328.000000\n",
            "Train Epoch: 12 [55040/60000 (92%)]\tLoss: -11551530549248.000000\n",
            "Train Epoch: 12 [55680/60000 (93%)]\tLoss: -11470513373184.000000\n",
            "Train Epoch: 12 [56320/60000 (94%)]\tLoss: -9353929162752.000000\n",
            "Train Epoch: 12 [56960/60000 (95%)]\tLoss: -12288596639744.000000\n",
            "Train Epoch: 12 [57600/60000 (96%)]\tLoss: -7333879480320.000000\n",
            "Train Epoch: 12 [58240/60000 (97%)]\tLoss: -11267746037760.000000\n",
            "Train Epoch: 12 [58880/60000 (98%)]\tLoss: -10202330955776.000000\n",
            "Train Epoch: 12 [59520/60000 (99%)]\tLoss: -11957916663808.000000\n",
            "\n",
            "Test set: Average loss: -10358329823566.2344, Accuracy: 3804/10000 (38%)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: -11440097329152.000000\n",
            "Train Epoch: 13 [640/60000 (1%)]\tLoss: -10021930795008.000000\n",
            "Train Epoch: 13 [1280/60000 (2%)]\tLoss: -10503970619392.000000\n",
            "Train Epoch: 13 [1920/60000 (3%)]\tLoss: -11027370475520.000000\n",
            "Train Epoch: 13 [2560/60000 (4%)]\tLoss: -10352802660352.000000\n",
            "Train Epoch: 13 [3200/60000 (5%)]\tLoss: -10679036674048.000000\n",
            "Train Epoch: 13 [3840/60000 (6%)]\tLoss: -11946601480192.000000\n",
            "Train Epoch: 13 [4480/60000 (7%)]\tLoss: -8626658869248.000000\n",
            "Train Epoch: 13 [5120/60000 (9%)]\tLoss: -12897735409664.000000\n",
            "Train Epoch: 13 [5760/60000 (10%)]\tLoss: -9508032086016.000000\n",
            "Train Epoch: 13 [6400/60000 (11%)]\tLoss: -11842913042432.000000\n",
            "Train Epoch: 13 [7040/60000 (12%)]\tLoss: -10357120696320.000000\n",
            "Train Epoch: 13 [7680/60000 (13%)]\tLoss: -11823093907456.000000\n",
            "Train Epoch: 13 [8320/60000 (14%)]\tLoss: -11320188469248.000000\n",
            "Train Epoch: 13 [8960/60000 (15%)]\tLoss: -8607353012224.000000\n",
            "Train Epoch: 13 [9600/60000 (16%)]\tLoss: -10537122398208.000000\n",
            "Train Epoch: 13 [10240/60000 (17%)]\tLoss: -13507275784192.000000\n",
            "Train Epoch: 13 [10880/60000 (18%)]\tLoss: -11443464306688.000000\n",
            "Train Epoch: 13 [11520/60000 (19%)]\tLoss: -10521120079872.000000\n",
            "Train Epoch: 13 [12160/60000 (20%)]\tLoss: -7761895096320.000000\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: -10023427112960.000000\n",
            "Train Epoch: 13 [13440/60000 (22%)]\tLoss: -8176235184128.000000\n",
            "Train Epoch: 13 [14080/60000 (23%)]\tLoss: -11779833856000.000000\n",
            "Train Epoch: 13 [14720/60000 (25%)]\tLoss: -10379744772096.000000\n",
            "Train Epoch: 13 [15360/60000 (26%)]\tLoss: -6849771864064.000000\n",
            "Train Epoch: 13 [16000/60000 (27%)]\tLoss: -8750922465280.000000\n",
            "Train Epoch: 13 [16640/60000 (28%)]\tLoss: -10092721209344.000000\n",
            "Train Epoch: 13 [17280/60000 (29%)]\tLoss: -11738199097344.000000\n",
            "Train Epoch: 13 [17920/60000 (30%)]\tLoss: -10783137202176.000000\n",
            "Train Epoch: 13 [18560/60000 (31%)]\tLoss: -12272276602880.000000\n",
            "Train Epoch: 13 [19200/60000 (32%)]\tLoss: -10827776131072.000000\n",
            "Train Epoch: 13 [19840/60000 (33%)]\tLoss: -11294153375744.000000\n",
            "Train Epoch: 13 [20480/60000 (34%)]\tLoss: -10841236701184.000000\n",
            "Train Epoch: 13 [21120/60000 (35%)]\tLoss: -9758476075008.000000\n",
            "Train Epoch: 13 [21760/60000 (36%)]\tLoss: -9116242149376.000000\n",
            "Train Epoch: 13 [22400/60000 (37%)]\tLoss: -11728500817920.000000\n",
            "Train Epoch: 13 [23040/60000 (38%)]\tLoss: -11394994929664.000000\n",
            "Train Epoch: 13 [23680/60000 (39%)]\tLoss: -9834377248768.000000\n",
            "Train Epoch: 13 [24320/60000 (41%)]\tLoss: -12358002933760.000000\n",
            "Train Epoch: 13 [24960/60000 (42%)]\tLoss: -11568322445312.000000\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: -11856265609216.000000\n",
            "Train Epoch: 13 [26240/60000 (44%)]\tLoss: -10610530058240.000000\n",
            "Train Epoch: 13 [26880/60000 (45%)]\tLoss: -10200576688128.000000\n",
            "Train Epoch: 13 [27520/60000 (46%)]\tLoss: -11229843161088.000000\n",
            "Train Epoch: 13 [28160/60000 (47%)]\tLoss: -8274241912832.000000\n",
            "Train Epoch: 13 [28800/60000 (48%)]\tLoss: -8767951339520.000000\n",
            "Train Epoch: 13 [29440/60000 (49%)]\tLoss: -6144686817280.000000\n",
            "Train Epoch: 13 [30080/60000 (50%)]\tLoss: -11219174948864.000000\n",
            "Train Epoch: 13 [30720/60000 (51%)]\tLoss: -10055916191744.000000\n",
            "Train Epoch: 13 [31360/60000 (52%)]\tLoss: -12099852959744.000000\n",
            "Train Epoch: 13 [32000/60000 (53%)]\tLoss: -10169839779840.000000\n",
            "Train Epoch: 13 [32640/60000 (54%)]\tLoss: -9541958762496.000000\n",
            "Train Epoch: 13 [33280/60000 (55%)]\tLoss: -10559613304832.000000\n",
            "Train Epoch: 13 [33920/60000 (57%)]\tLoss: -8088578424832.000000\n",
            "Train Epoch: 13 [34560/60000 (58%)]\tLoss: -11336172961792.000000\n",
            "Train Epoch: 13 [35200/60000 (59%)]\tLoss: -8355472474112.000000\n",
            "Train Epoch: 13 [35840/60000 (60%)]\tLoss: -11891054215168.000000\n",
            "Train Epoch: 13 [36480/60000 (61%)]\tLoss: -11263845335040.000000\n",
            "Train Epoch: 13 [37120/60000 (62%)]\tLoss: -9804471861248.000000\n",
            "Train Epoch: 13 [37760/60000 (63%)]\tLoss: -12630699802624.000000\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: -10192334880768.000000\n",
            "Train Epoch: 13 [39040/60000 (65%)]\tLoss: -10827961729024.000000\n",
            "Train Epoch: 13 [39680/60000 (66%)]\tLoss: -9496950734848.000000\n",
            "Train Epoch: 13 [40320/60000 (67%)]\tLoss: -10265561137152.000000\n",
            "Train Epoch: 13 [40960/60000 (68%)]\tLoss: -11227193409536.000000\n",
            "Train Epoch: 13 [41600/60000 (69%)]\tLoss: -10106712358912.000000\n",
            "Train Epoch: 13 [42240/60000 (70%)]\tLoss: -7647422578688.000000\n",
            "Train Epoch: 13 [42880/60000 (71%)]\tLoss: -11675664121856.000000\n",
            "Train Epoch: 13 [43520/60000 (72%)]\tLoss: -10423378116608.000000\n",
            "Train Epoch: 13 [44160/60000 (74%)]\tLoss: -12110040924160.000000\n",
            "Train Epoch: 13 [44800/60000 (75%)]\tLoss: -11097764528128.000000\n",
            "Train Epoch: 13 [45440/60000 (76%)]\tLoss: -10790967967744.000000\n",
            "Train Epoch: 13 [46080/60000 (77%)]\tLoss: -12258996387840.000000\n",
            "Train Epoch: 13 [46720/60000 (78%)]\tLoss: -12950938058752.000000\n",
            "Train Epoch: 13 [47360/60000 (79%)]\tLoss: -9847317725184.000000\n",
            "Train Epoch: 13 [48000/60000 (80%)]\tLoss: -10623822856192.000000\n",
            "Train Epoch: 13 [48640/60000 (81%)]\tLoss: -11006997692416.000000\n",
            "Train Epoch: 13 [49280/60000 (82%)]\tLoss: -12967769800704.000000\n",
            "Train Epoch: 13 [49920/60000 (83%)]\tLoss: -10268818014208.000000\n",
            "Train Epoch: 13 [50560/60000 (84%)]\tLoss: -9386258857984.000000\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: -9910725115904.000000\n",
            "Train Epoch: 13 [51840/60000 (86%)]\tLoss: -8749701398528.000000\n",
            "Train Epoch: 13 [52480/60000 (87%)]\tLoss: -11207590281216.000000\n",
            "Train Epoch: 13 [53120/60000 (88%)]\tLoss: -11654978863104.000000\n",
            "Train Epoch: 13 [53760/60000 (90%)]\tLoss: -9077700689920.000000\n",
            "Train Epoch: 13 [54400/60000 (91%)]\tLoss: -10219158503424.000000\n",
            "Train Epoch: 13 [55040/60000 (92%)]\tLoss: -10352256352256.000000\n",
            "Train Epoch: 13 [55680/60000 (93%)]\tLoss: -12143245131776.000000\n",
            "Train Epoch: 13 [56320/60000 (94%)]\tLoss: -11060220264448.000000\n",
            "Train Epoch: 13 [56960/60000 (95%)]\tLoss: -11047155007488.000000\n",
            "Train Epoch: 13 [57600/60000 (96%)]\tLoss: -9504006602752.000000\n",
            "Train Epoch: 13 [58240/60000 (97%)]\tLoss: -8998146277376.000000\n",
            "Train Epoch: 13 [58880/60000 (98%)]\tLoss: -12720835395584.000000\n",
            "Train Epoch: 13 [59520/60000 (99%)]\tLoss: -9923169615872.000000\n",
            "\n",
            "Test set: Average loss: -10433714552674.7129, Accuracy: 3807/10000 (38%)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: -5329800658944.000000\n",
            "Train Epoch: 14 [640/60000 (1%)]\tLoss: -11611477639168.000000\n",
            "Train Epoch: 14 [1280/60000 (2%)]\tLoss: -9641466527744.000000\n",
            "Train Epoch: 14 [1920/60000 (3%)]\tLoss: -9968935763968.000000\n",
            "Train Epoch: 14 [2560/60000 (4%)]\tLoss: -11536905011200.000000\n",
            "Train Epoch: 14 [3200/60000 (5%)]\tLoss: -9448163639296.000000\n",
            "Train Epoch: 14 [3840/60000 (6%)]\tLoss: -12100526145536.000000\n",
            "Train Epoch: 14 [4480/60000 (7%)]\tLoss: -11265010302976.000000\n",
            "Train Epoch: 14 [5120/60000 (9%)]\tLoss: -9289474244608.000000\n",
            "Train Epoch: 14 [5760/60000 (10%)]\tLoss: -9569560428544.000000\n",
            "Train Epoch: 14 [6400/60000 (11%)]\tLoss: -7976350908416.000000\n",
            "Train Epoch: 14 [7040/60000 (12%)]\tLoss: -11392997392384.000000\n",
            "Train Epoch: 14 [7680/60000 (13%)]\tLoss: -8923827404800.000000\n",
            "Train Epoch: 14 [8320/60000 (14%)]\tLoss: -10521453527040.000000\n",
            "Train Epoch: 14 [8960/60000 (15%)]\tLoss: -7611150237696.000000\n",
            "Train Epoch: 14 [9600/60000 (16%)]\tLoss: -11530516037632.000000\n",
            "Train Epoch: 14 [10240/60000 (17%)]\tLoss: -12037265555456.000000\n",
            "Train Epoch: 14 [10880/60000 (18%)]\tLoss: -9160199503872.000000\n",
            "Train Epoch: 14 [11520/60000 (19%)]\tLoss: -9907990429696.000000\n",
            "Train Epoch: 14 [12160/60000 (20%)]\tLoss: -12080257171456.000000\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: -10894054522880.000000\n",
            "Train Epoch: 14 [13440/60000 (22%)]\tLoss: -8479534219264.000000\n",
            "Train Epoch: 14 [14080/60000 (23%)]\tLoss: -10128728260608.000000\n",
            "Train Epoch: 14 [14720/60000 (25%)]\tLoss: -11101256286208.000000\n",
            "Train Epoch: 14 [15360/60000 (26%)]\tLoss: -8883410042880.000000\n",
            "Train Epoch: 14 [16000/60000 (27%)]\tLoss: -12004077076480.000000\n",
            "Train Epoch: 14 [16640/60000 (28%)]\tLoss: -10005923233792.000000\n",
            "Train Epoch: 14 [17280/60000 (29%)]\tLoss: -10758344671232.000000\n",
            "Train Epoch: 14 [17920/60000 (30%)]\tLoss: -11367219200000.000000\n",
            "Train Epoch: 14 [18560/60000 (31%)]\tLoss: -8427200839680.000000\n",
            "Train Epoch: 14 [19200/60000 (32%)]\tLoss: -10259207815168.000000\n",
            "Train Epoch: 14 [19840/60000 (33%)]\tLoss: -11956835581952.000000\n",
            "Train Epoch: 14 [20480/60000 (34%)]\tLoss: -10805160443904.000000\n",
            "Train Epoch: 14 [21120/60000 (35%)]\tLoss: -12564947795968.000000\n",
            "Train Epoch: 14 [21760/60000 (36%)]\tLoss: -10062681604096.000000\n",
            "Train Epoch: 14 [22400/60000 (37%)]\tLoss: -8579147890688.000000\n",
            "Train Epoch: 14 [23040/60000 (38%)]\tLoss: -11418591035392.000000\n",
            "Train Epoch: 14 [23680/60000 (39%)]\tLoss: -9714995822592.000000\n",
            "Train Epoch: 14 [24320/60000 (41%)]\tLoss: -11486347919360.000000\n",
            "Train Epoch: 14 [24960/60000 (42%)]\tLoss: -10660877434880.000000\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: -10982635077632.000000\n",
            "Train Epoch: 14 [26240/60000 (44%)]\tLoss: -12193269547008.000000\n",
            "Train Epoch: 14 [26880/60000 (45%)]\tLoss: -10872158158848.000000\n",
            "Train Epoch: 14 [27520/60000 (46%)]\tLoss: -10265077743616.000000\n",
            "Train Epoch: 14 [28160/60000 (47%)]\tLoss: -11840809598976.000000\n",
            "Train Epoch: 14 [28800/60000 (48%)]\tLoss: -12299934892032.000000\n",
            "Train Epoch: 14 [29440/60000 (49%)]\tLoss: -8178229051392.000000\n",
            "Train Epoch: 14 [30080/60000 (50%)]\tLoss: -10782544756736.000000\n",
            "Train Epoch: 14 [30720/60000 (51%)]\tLoss: -8685234946048.000000\n",
            "Train Epoch: 14 [31360/60000 (52%)]\tLoss: -11800603000832.000000\n",
            "Train Epoch: 14 [32000/60000 (53%)]\tLoss: -9531959541760.000000\n",
            "Train Epoch: 14 [32640/60000 (54%)]\tLoss: -9835811700736.000000\n",
            "Train Epoch: 14 [33280/60000 (55%)]\tLoss: -12131752738816.000000\n",
            "Train Epoch: 14 [33920/60000 (57%)]\tLoss: -10825748185088.000000\n",
            "Train Epoch: 14 [34560/60000 (58%)]\tLoss: -10399037521920.000000\n",
            "Train Epoch: 14 [35200/60000 (59%)]\tLoss: -10574790393856.000000\n",
            "Train Epoch: 14 [35840/60000 (60%)]\tLoss: -11537279352832.000000\n",
            "Train Epoch: 14 [36480/60000 (61%)]\tLoss: -13666209497088.000000\n",
            "Train Epoch: 14 [37120/60000 (62%)]\tLoss: -9873070751744.000000\n",
            "Train Epoch: 14 [37760/60000 (63%)]\tLoss: -10850346729472.000000\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: -11165415505920.000000\n",
            "Train Epoch: 14 [39040/60000 (65%)]\tLoss: -10126359527424.000000\n",
            "Train Epoch: 14 [39680/60000 (66%)]\tLoss: -8686688796672.000000\n",
            "Train Epoch: 14 [40320/60000 (67%)]\tLoss: -7582399332352.000000\n",
            "Train Epoch: 14 [40960/60000 (68%)]\tLoss: -9779296600064.000000\n",
            "Train Epoch: 14 [41600/60000 (69%)]\tLoss: -10982124421120.000000\n",
            "Train Epoch: 14 [42240/60000 (70%)]\tLoss: -10114412052480.000000\n",
            "Train Epoch: 14 [42880/60000 (71%)]\tLoss: -11531420958720.000000\n",
            "Train Epoch: 14 [43520/60000 (72%)]\tLoss: -7469662732288.000000\n",
            "Train Epoch: 14 [44160/60000 (74%)]\tLoss: -11200572162048.000000\n",
            "Train Epoch: 14 [44800/60000 (75%)]\tLoss: -10558180950016.000000\n",
            "Train Epoch: 14 [45440/60000 (76%)]\tLoss: -8080759717888.000000\n",
            "Train Epoch: 14 [46080/60000 (77%)]\tLoss: -9586552602624.000000\n",
            "Train Epoch: 14 [46720/60000 (78%)]\tLoss: -10034446598144.000000\n",
            "Train Epoch: 14 [47360/60000 (79%)]\tLoss: -7819022565376.000000\n",
            "Train Epoch: 14 [48000/60000 (80%)]\tLoss: -6988300812288.000000\n",
            "Train Epoch: 14 [48640/60000 (81%)]\tLoss: -11681768931328.000000\n",
            "Train Epoch: 14 [49280/60000 (82%)]\tLoss: -10853315248128.000000\n",
            "Train Epoch: 14 [49920/60000 (83%)]\tLoss: -11593277505536.000000\n",
            "Train Epoch: 14 [50560/60000 (84%)]\tLoss: -10368597360640.000000\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: -12990129635328.000000\n",
            "Train Epoch: 14 [51840/60000 (86%)]\tLoss: -9576451670016.000000\n",
            "Train Epoch: 14 [52480/60000 (87%)]\tLoss: -8186271105024.000000\n",
            "Train Epoch: 14 [53120/60000 (88%)]\tLoss: -10384586047488.000000\n",
            "Train Epoch: 14 [53760/60000 (90%)]\tLoss: -9875337773056.000000\n",
            "Train Epoch: 14 [54400/60000 (91%)]\tLoss: -10728192868352.000000\n",
            "Train Epoch: 14 [55040/60000 (92%)]\tLoss: -9482547494912.000000\n",
            "Train Epoch: 14 [55680/60000 (93%)]\tLoss: -12044566790144.000000\n",
            "Train Epoch: 14 [56320/60000 (94%)]\tLoss: -9287057276928.000000\n",
            "Train Epoch: 14 [56960/60000 (95%)]\tLoss: -10043458060288.000000\n",
            "Train Epoch: 14 [57600/60000 (96%)]\tLoss: -10264093130752.000000\n",
            "Train Epoch: 14 [58240/60000 (97%)]\tLoss: -11440261955584.000000\n",
            "Train Epoch: 14 [58880/60000 (98%)]\tLoss: -11351326982144.000000\n",
            "Train Epoch: 14 [59520/60000 (99%)]\tLoss: -9930762354688.000000\n",
            "\n",
            "Test set: Average loss: -10237832959151.3086, Accuracy: 3814/10000 (38%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrGGPuzJTvFl",
        "colab_type": "code",
        "outputId": "7192d7c3-ae20-4f77-f7a1-b0d87cfb573c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.3.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayFLgOxKUKAU",
        "colab_type": "code",
        "outputId": "da66bf8a-6c29-458e-cd77-d5636c149a9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torchvision\n",
        "torchvision.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.4.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao-L8lY3UNXE",
        "colab_type": "code",
        "outputId": "da1530ed-7190-41a7-e9ad-201de1198689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.17.4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itQP5ZjyUSET",
        "colab_type": "code",
        "outputId": "6760c991-c6d9-41ba-cf25-5a70f9b1d5fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import sys\n",
        "sys.version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.6.9 (default, Nov  7 2019, 10:44:02) \\n[GCC 8.3.0]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbSmeZ7dVRZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}